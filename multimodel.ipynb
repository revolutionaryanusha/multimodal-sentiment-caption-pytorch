{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOsnx6yF46y7",
        "outputId": "a8429192-f09c-4461-8692-ea0a55a0b5ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NplsnP3Q5Spv"
      },
      "source": [
        "PART1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mlYtJW6_5TmZ",
        "outputId": "4f7bd8da-e808-4cf9-9265-e77e566ef296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAHWCAYAAACFXRQ+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABEdUlEQVR4nO3deVhWdf7/8dcNet+4AW5siUharohlDtLiyohENk42pVkuoVaDmdLXHMoh1MpGcystx2mUmrRcaqzUVMSFSWmRwq3kp4bRjIKWwi1WIHB+f8yX8/U+uBJyoz0f13Wui/M57/tz3ue28MXxcx9shmEYAgAAAGDycHcDAAAAQG1DSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBlDrJScny2az1ci5evXqpV69epn7W7dulc1m06pVq2rk/CNGjFCrVq1q5FxVVVRUpFGjRikgIEA2m03jx493d0uX5fDhw7LZbEpJSXF3KwBqMUIygBqVkpIim81mbl5eXgoKClJ0dLRefvllnTp1qlrOc+TIESUnJysrK6ta5qtOtbm3S/HCCy8oJSVFjz32mP7xj3/ooYceOm9tSUmJ5s2bp5tuukne3t7y9fVVx44dNWbMGO3fv/+K9rls2TLNnTv3ip7jSlq3bp2Sk5Pd3Qbwq2UzDMNwdxMAfj1SUlI0cuRITZ06VaGhoTpz5ozy8vK0detWpaamqmXLlvrggw/UuXNn8zWlpaUqLS2Vl5fXJZ9n586d6tatm5YsWaIRI0Zc8utKSkokSXa7XdJ/7yT37t1bK1eu1L333nvJ81S1tzNnzqi8vFwOh6NaznUldO/eXXXq1NHHH3980doBAwboo48+0pAhQxQZGakzZ85o//79WrNmjaZNm3ZZfzaX66677tLevXt1+PBhl3HDMFRcXKy6devK09Pzip3/lxo7dqwWLFgg/poG3KOOuxsA8OsUExOjW265xdxPTEzU5s2bddddd+nuu+/W119/rXr16kmS6tSpozp1ruy3qx9//FH169c3w7G71K1b163nvxTHjh1Thw4dLlr3+eefa82aNXr++ef19NNPuxybP3++CgoKrlCHF1bxLxgAcCEstwBQa/Tp00d//vOf9e233+qtt94yx8+1Jjk1NVW33367fH191bBhQ7Vt29YMYlu3blW3bt0kSSNHjjSXdlSsQe3Vq5c6deqkzMxM9ejRQ/Xr1zdfa12TXKGsrExPP/20AgIC1KBBA91999367rvvXGpatWp1zjujZ895sd7OtSb59OnTevLJJxUcHCyHw6G2bdvqpZdeqnSH0WazaezYsVq9erU6deokh8Ohjh07av369ed+wy2OHTumuLg4+fv7y8vLS+Hh4XrjjTfM4xXrs3NycrR27Vqzd+ud2gqHDh2SJN12222Vjnl6eqpp06YuY//5z3/08MMPy9/f3+x98eLFLjUVPaxYsULPP/+8WrRoIS8vL/Xt21cHDx4063r16qW1a9fq22+/NfuseF/PtSZ5xIgRatiwoXJzc3XXXXepYcOGuu6667RgwQJJ0p49e9SnTx81aNBAISEhWrZsWaVrKigo0Pjx480/pzZt2ugvf/mLysvLzZqKc7/00ktatGiRWrduLYfDoW7duunzzz936afi3GcvT6rwzjvvqGvXrmrUqJG8vb0VFhamefPmnfPPAUDVcCcZQK3y0EMP6emnn9bGjRs1evToc9bs27dPd911lzp37qypU6fK4XDo4MGD2r59uySpffv2mjp1qpKSkjRmzBjdcccdkqRbb73VnOOHH35QTEyMBg8erAcffFD+/v4X7Ov555+XzWbTpEmTdOzYMc2dO1dRUVHKysoy73hfikvp7WyGYejuu+/Wli1bFBcXpy5dumjDhg2aOHGi/vOf/2jOnDku9R9//LHee+89/fGPf1SjRo308ssva9CgQcrNza0USs/2008/qVevXjp48KDGjh2r0NBQrVy5UiNGjFBBQYGeeOIJtW/fXv/4xz80YcIEtWjRQk8++aQkqXnz5uecMyQkRJK0dOlS3XbbbRf814D8/Hx1797dDPrNmzfXRx99pLi4ODmdzkofDnzxxRfl4eGh//mf/1FhYaFmzJihoUOH6tNPP5UkPfPMMyosLNS///1v8z1q2LDhec8v/fcHoZiYGPXo0UMzZszQ0qVLNXbsWDVo0EDPPPOMhg4dqnvuuUcLFy7UsGHDFBkZqdDQUEn//ZeInj176j//+Y8eeeQRtWzZUjt27FBiYqKOHj1aaW30smXLdOrUKT3yyCOy2WyaMWOG7rnnHn3zzTeqW7euHnnkER05ckSpqan6xz/+4fLa1NRUDRkyRH379tVf/vIXSdLXX3+t7du364knnrjgNQK4DAYA1KAlS5YYkozPP//8vDU+Pj7GTTfdZO4/++yzxtnfrubMmWNIMo4fP37eOT7//HNDkrFkyZJKx3r27GlIMhYuXHjOYz179jT3t2zZYkgyrrvuOsPpdJrjK1asMCQZ8+bNM8dCQkKM4cOHX3TOC/U2fPhwIyQkxNxfvXq1Icl47rnnXOruvfdew2azGQcPHjTHJBl2u91lbNeuXYYk45VXXql0rrPNnTvXkGS89dZb5lhJSYkRGRlpNGzY0OXaQ0JCjNjY2AvOZxiGUV5ebr7X/v7+xpAhQ4wFCxYY3377baXauLg4IzAw0Pj+++9dxgcPHmz4+PgYP/74o2EY//fn0b59e6O4uNismzdvniHJ2LNnjzkWGxvr8l5WyMnJqfT+Dx8+3JBkvPDCC+bYyZMnjXr16hk2m8145513zPH9+/cbkoxnn33WHJs2bZrRoEED4//9v//ncq4//elPhqenp5Gbm+ty7qZNmxonTpww695//31DkvHhhx+aY/Hx8ca5/pp+4oknDG9vb6O0tLTSMQDVh+UWAGqdhg0bXvApF76+vpKk999/3+Wfsi+Hw+HQyJEjL7l+2LBhatSokbl/7733KjAwUOvWravS+S/VunXr5OnpqXHjxrmMP/nkkzIMQx999JHLeFRUlFq3bm3ud+7cWd7e3vrmm28uep6AgAANGTLEHKtbt67GjRunoqIibdu27bJ7t9ls2rBhg5577jk1btxYb7/9tuLj4xUSEqL777/fXJNsGIbeffddDRgwQIZh6Pvvvze36OhoFRYW6osvvnCZe+TIkS7rxyvuyF/sOi9m1KhR5te+vr5q27atGjRooPvuu88cb9u2rXx9fV3OtXLlSt1xxx1q3LixS/9RUVEqKytTenq6y3nuv/9+NW7cuEr9+/r66vTp00pNTa3ydQK4OEIygFqnqKjIJZBa3X///brttts0atQo+fv7a/DgwVqxYsVlBebrrrvusj6kd8MNN7js22w2tWnT5rzrcavLt99+q6CgoErvR/v27c3jZ2vZsmWlORo3bqyTJ09e9Dw33HCDPDxc/1o433kulcPh0DPPPKOvv/5aR44c0dtvv63u3btrxYoVGjt2rCTp+PHjKigo0KJFi9S8eXOXreIHmWPHjl3wOisC58Wu80K8vLwqLR3x8fFRixYtKq2J9/HxcTnXgQMHtH79+kr9R0VFVXv/f/zjH3XjjTcqJiZGLVq00MMPP3zJ684BXDrWJAOoVf7973+rsLBQbdq0OW9NvXr1lJ6eri1btmjt2rVav369li9frj59+mjjxo2X9Fivy1lHfKnO9wtPysrKauxRY+c7j1ELHiMWGBiowYMHa9CgQerYsaNWrFihlJQU84ebBx98UMOHDz/na89+JKB0Za7zfHNeyrnKy8v129/+Vk899dQ5a2+88cbLnvN8/Pz8lJWVpQ0bNuijjz7SRx99pCVLlmjYsGEuH7QE8MsQkgHUKhUfUoqOjr5gnYeHh/r27au+fftq9uzZeuGFF/TMM89oy5YtioqKqvbf0HfgwAGXfcMwdPDgQZfw1rhx43M+1uzbb7/V9ddfb+5fTm8hISHatGmTTp065XI3ueIXcVR8OO6XCgkJ0e7du1VeXu5yN7m6zyP9dxlH586ddeDAAX3//fdq3ry5GjVqpLKyMvPOa3Woqd/SKEmtW7dWUVFRjfVvt9s1YMAADRgwQOXl5frjH/+ov/71r/rzn/98wR8wAVw6llsAqDU2b96sadOmKTQ0VEOHDj1v3YkTJyqNdenSRZJUXFwsSWrQoIEkVduzeN98802XddKrVq3S0aNHFRMTY461bt1an3zyifkLSSRpzZo1lR4Vdzm93XnnnSorK9P8+fNdxufMmSObzeZy/l/izjvvVF5enpYvX26OlZaW6pVXXlHDhg3Vs2fPy57zwIEDys3NrTReUFCgjIwMNW7cWM2bN5enp6cGDRqkd999V3v37q1Uf/z48cs+t/Tf97mwsLBKr71c9913nzIyMrRhw4ZKxwoKClRaWnrZc57vv5MffvjBZd/Dw8P8Ya3iv38Avxx3kgG4xUcffaT9+/ertLRU+fn52rx5s1JTUxUSEqIPPvjggr/sYerUqUpPT1dsbKxCQkJ07Ngxvfrqq2rRooVuv/12Sf8NrL6+vlq4cKEaNWqkBg0aKCIiwnxk1+Vq0qSJbr/9do0cOVL5+fmaO3eu2rRp4/KYulGjRmnVqlXq37+/7rvvPh06dEhvvfWWywfpLre3AQMGqHfv3nrmmWd0+PBhhYeHa+PGjXr//fc1fvz4SnNX1ZgxY/TXv/5VI0aMUGZmplq1aqVVq1Zp+/btmjt37gXXiJ/Prl279MADDygmJkZ33HGHmjRpov/85z964403dOTIEc2dO9dcdvDiiy9qy5YtioiI0OjRo9WhQwedOHFCX3zxhTZt2nTOH4wupmvXrlq+fLkSEhLUrVs3NWzYUAMGDLjseS7FxIkT9cEHH+iuu+7SiBEj1LVrV50+fVp79uzRqlWrdPjwYTVr1uyy+5ekcePGKTo6Wp6enho8eLBGjRqlEydOqE+fPmrRooW+/fZbvfLKK+rSpYu5hhxANXDfgzUA/BpVPAKuYrPb7UZAQIDx29/+1pg3b57Lo8YqWB8Bl5aWZvzud78zgoKCDLvdbgQFBRlDhgyp9Pit999/3+jQoYNRp04dl0d+9ezZ0+jYseM5+zvfI+DefvttIzEx0fDz8zPq1atnxMbGnvNRZrNmzTKuu+46w+FwGLfddpuxc+fOSnNeqDfrI+AMwzBOnTplTJgwwQgKCjLq1q1r3HDDDcbMmTON8vJylzpJRnx8fKWezvdoOqv8/Hxj5MiRRrNmzQy73W6EhYWd8zF1l/oIuPz8fOPFF180evbsaQQGBhp16tQxGjdubPTp08dYtWrVOevj4+ON4OBgo27dukZAQIDRt29fY9GiRWZNxZ/HypUrXV57rse6FRUVGQ888IDh6+trSDLf1/M9Aq5BgwaVejrffyvneg9OnTplJCYmGm3atDHsdrvRrFkz49ZbbzVeeuklo6SkxOXcM2fOrDSnLI+VKy0tNR5//HGjefPmhs1mM/8fWLVqldGvXz/Dz8/PsNvtRsuWLY1HHnnEOHr0aKU5AVSdzTBqwac5AAAAgFqENckAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC36ZSDUpLy/XkSNH1KhRoxr9VagAAAC4NIZh6NSpUwoKCpKHx4XvFROSq8mRI0cUHBzs7jYAAABwEd99951atGhxwRpCcjWp+JWt3333nby9vd3cDQAAAKycTqeCg4PN3HYhhORqUrHEwtvbm5AMAABQi13K0lg+uAcAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWddzdAAAA55I7NczdLQC4Qlom7XF3CxfFnWQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwMKtIXn69Onq1q2bGjVqJD8/Pw0cOFDZ2dkuNT///LPi4+PVtGlTNWzYUIMGDVJ+fr5LTW5urmJjY1W/fn35+flp4sSJKi0tdanZunWrbr75ZjkcDrVp00YpKSmV+lmwYIFatWolLy8vRURE6LPPPqv2awYAAEDt59aQvG3bNsXHx+uTTz5Ramqqzpw5o379+un06dNmzYQJE/Thhx9q5cqV2rZtm44cOaJ77rnHPF5WVqbY2FiVlJRox44deuONN5SSkqKkpCSzJicnR7Gxserdu7eysrI0fvx4jRo1Shs2bDBrli9froSEBD377LP64osvFB4erujoaB07dqxm3gwAAADUGjbDMAx3N1Hh+PHj8vPz07Zt29SjRw8VFhaqefPmWrZsme69915J0v79+9W+fXtlZGSoe/fu+uijj3TXXXfpyJEj8vf3lyQtXLhQkyZN0vHjx2W32zVp0iStXbtWe/fuNc81ePBgFRQUaP369ZKkiIgIdevWTfPnz5cklZeXKzg4WI8//rj+9Kc/XbR3p9MpHx8fFRYWytvbu7rfGgD41cmdGubuFgBcIS2T9rjlvJeT12rVmuTCwkJJUpMmTSRJmZmZOnPmjKKiosyadu3aqWXLlsrIyJAkZWRkKCwszAzIkhQdHS2n06l9+/aZNWfPUVFTMUdJSYkyMzNdajw8PBQVFWXWWBUXF8vpdLpsAAAAuDbUmpBcXl6u8ePH67bbblOnTp0kSXl5ebLb7fL19XWp9ff3V15enllzdkCuOF5x7EI1TqdTP/30k77//nuVlZWds6ZiDqvp06fLx8fH3IKDg6t24QAAAKh1ak1Ijo+P1969e/XOO++4u5VLkpiYqMLCQnP77rvv3N0SAAAAqkkddzcgSWPHjtWaNWuUnp6uFi1amOMBAQEqKSlRQUGBy93k/Px8BQQEmDXWp1BUPP3i7BrrEzHy8/Pl7e2tevXqydPTU56enuesqZjDyuFwyOFwVO2CAQAAUKu59U6yYRgaO3as/vnPf2rz5s0KDQ11Od61a1fVrVtXaWlp5lh2drZyc3MVGRkpSYqMjNSePXtcnkKRmpoqb29vdejQwaw5e46Kmoo57Ha7unbt6lJTXl6utLQ0swYAAAC/Hm69kxwfH69ly5bp/fffV6NGjcz1vz4+PqpXr558fHwUFxenhIQENWnSRN7e3nr88ccVGRmp7t27S5L69eunDh066KGHHtKMGTOUl5enyZMnKz4+3rzT++ijj2r+/Pl66qmn9PDDD2vz5s1asWKF1q5da/aSkJCg4cOH65ZbbtFvfvMbzZ07V6dPn9bIkSNr/o0BAACAW7k1JL/22muSpF69ermML1myRCNGjJAkzZkzRx4eHho0aJCKi4sVHR2tV1991az19PTUmjVr9NhjjykyMlINGjTQ8OHDNXXqVLMmNDRUa9eu1YQJEzRv3jy1aNFCr7/+uqKjo82a+++/X8ePH1dSUpLy8vLUpUsXrV+/vtKH+QAAAHDtq1XPSb6a8ZxkAKhePCcZuHbxnGQAAADgKkRIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFnXc3QCqR9eJb7q7BQBXSObMYe5uAQB+dbiTDAAAAFgQkgEAAAALt4bk9PR0DRgwQEFBQbLZbFq9erXLcZvNds5t5syZZk2rVq0qHX/xxRdd5tm9e7fuuOMOeXl5KTg4WDNmzKjUy8qVK9WuXTt5eXkpLCxM69atuyLXDAAAgNrPrSH59OnTCg8P14IFC855/OjRoy7b4sWLZbPZNGjQIJe6qVOnutQ9/vjj5jGn06l+/fopJCREmZmZmjlzppKTk7Vo0SKzZseOHRoyZIji4uL05ZdfauDAgRo4cKD27t17ZS4cAAAAtZpbP7gXExOjmJiY8x4PCAhw2X///ffVu3dvXX/99S7jjRo1qlRbYenSpSopKdHixYtlt9vVsWNHZWVlafbs2RozZowkad68eerfv78mTpwoSZo2bZpSU1M1f/58LVy48JdcIgAAAK5CV82a5Pz8fK1du1ZxcXGVjr344otq2rSpbrrpJs2cOVOlpaXmsYyMDPXo0UN2u90ci46OVnZ2tk6ePGnWREVFucwZHR2tjIyM8/ZTXFwsp9PpsgEAAODacNU8Au6NN95Qo0aNdM8997iMjxs3TjfffLOaNGmiHTt2KDExUUePHtXs2bMlSXl5eQoNDXV5jb+/v3mscePGysvLM8fOrsnLyztvP9OnT9eUKVOq49IAAABQy1w1IXnx4sUaOnSovLy8XMYTEhLMrzt37iy73a5HHnlE06dPl8PhuGL9JCYmupzb6XQqODj4ip0PAAAANeeqCMn/+te/lJ2dreXLl1+0NiIiQqWlpTp8+LDatm2rgIAA5efnu9RU7FesYz5fzfnWOUuSw+G4oiEcAAAA7nNVrEn++9//rq5duyo8PPyitVlZWfLw8JCfn58kKTIyUunp6Tpz5oxZk5qaqrZt26px48ZmTVpamss8qampioyMrMarAAAAwNXCrSG5qKhIWVlZysrKkiTl5OQoKytLubm5Zo3T6dTKlSs1atSoSq/PyMjQ3LlztWvXLn3zzTdaunSpJkyYoAcffNAMwA888IDsdrvi4uK0b98+LV++XPPmzXNZKvHEE09o/fr1mjVrlvbv36/k5GTt3LlTY8eOvbJvAAAAAGolty632Llzp3r37m3uVwTX4cOHKyUlRZL0zjvvyDAMDRkypNLrHQ6H3nnnHSUnJ6u4uFihoaGaMGGCSwD28fHRxo0bFR8fr65du6pZs2ZKSkoyH/8mSbfeequWLVumyZMn6+mnn9YNN9yg1atXq1OnTlfoygEAAFCb2QzDMNzdxLXA6XTKx8dHhYWF8vb2rvHzd534Zo2fE0DNyJw5zN0tuEXu1DB3twDgCmmZtMct572cvHZVrEkGAAAAahIhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAs3BqS09PTNWDAAAUFBclms2n16tUux0eMGCGbzeay9e/f36XmxIkTGjp0qLy9veXr66u4uDgVFRW51OzevVt33HGHvLy8FBwcrBkzZlTqZeXKlWrXrp28vLwUFhamdevWVfv1AgAA4Org1pB8+vRphYeHa8GCBeet6d+/v44ePWpub7/9tsvxoUOHat++fUpNTdWaNWuUnp6uMWPGmMedTqf69eunkJAQZWZmaubMmUpOTtaiRYvMmh07dmjIkCGKi4vTl19+qYEDB2rgwIHau3dv9V80AAAAar067jx5TEyMYmJiLljjcDgUEBBwzmNff/211q9fr88//1y33HKLJOmVV17RnXfeqZdeeklBQUFaunSpSkpKtHjxYtntdnXs2FFZWVmaPXu2GabnzZun/v37a+LEiZKkadOmKTU1VfPnz9fChQur8YoBAABwNaj1a5K3bt0qPz8/tW3bVo899ph++OEH81hGRoZ8fX3NgCxJUVFR8vDw0KeffmrW9OjRQ3a73ayJjo5Wdna2Tp48adZERUW5nDc6OloZGRnn7au4uFhOp9NlAwAAwLWhVofk/v37680331RaWpr+8pe/aNu2bYqJiVFZWZkkKS8vT35+fi6vqVOnjpo0aaK8vDyzxt/f36WmYv9iNRXHz2X69Ony8fExt+Dg4F92sQAAAKg13Lrc4mIGDx5sfh0WFqbOnTurdevW2rp1q/r27evGzqTExEQlJCSY+06nk6AMAABwjajVd5Ktrr/+ejVr1kwHDx6UJAUEBOjYsWMuNaWlpTpx4oS5jjkgIED5+fkuNRX7F6s531po6b9rpb29vV02AAAAXBuuqpD873//Wz/88IMCAwMlSZGRkSooKFBmZqZZs3nzZpWXlysiIsKsSU9P15kzZ8ya1NRUtW3bVo0bNzZr0tLSXM6VmpqqyMjIK31JAAAAqIXcGpKLioqUlZWlrKwsSVJOTo6ysrKUm5uroqIiTZw4UZ988okOHz6stLQ0/e53v1ObNm0UHR0tSWrfvr369++v0aNH67PPPtP27ds1duxYDR48WEFBQZKkBx54QHa7XXFxcdq3b5+WL1+uefPmuSyVeOKJJ7R+/XrNmjVL+/fvV3Jysnbu3KmxY8fW+HsCAAAA93NrSN65c6duuukm3XTTTZKkhIQE3XTTTUpKSpKnp6d2796tu+++WzfeeKPi4uLUtWtX/etf/5LD4TDnWLp0qdq1a6e+ffvqzjvv1O233+7yDGQfHx9t3LhROTk56tq1q5588kklJSW5PEv51ltv1bJly7Ro0SKFh4dr1apVWr16tTp16lRzbwYAAABqDZthGIa7m7gWOJ1O+fj4qLCw0C3rk7tOfLPGzwmgZmTOHObuFtwid2qYu1sAcIW0TNrjlvNeTl67qtYkAwAAADWBkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFm4Nyenp6RowYICCgoJks9m0evVq89iZM2c0adIkhYWFqUGDBgoKCtKwYcN05MgRlzlatWolm83msr344osuNbt379Ydd9whLy8vBQcHa8aMGZV6Wblypdq1aycvLy+FhYVp3bp1V+SaAQAAUPu5NSSfPn1a4eHhWrBgQaVjP/74o7744gv9+c9/1hdffKH33ntP2dnZuvvuuyvVTp06VUePHjW3xx9/3DzmdDrVr18/hYSEKDMzUzNnzlRycrIWLVpk1uzYsUNDhgxRXFycvvzySw0cOFADBw7U3r17r8yFAwAAoFar486Tx8TEKCYm5pzHfHx8lJqa6jI2f/58/eY3v1Fubq5atmxpjjdq1EgBAQHnnGfp0qUqKSnR4sWLZbfb1bFjR2VlZWn27NkaM2aMJGnevHnq37+/Jk6cKEmaNm2aUlNTNX/+fC1cuLA6LhUAAABXkatqTXJhYaFsNpt8fX1dxl988UU1bdpUN910k2bOnKnS0lLzWEZGhnr06CG73W6ORUdHKzs7WydPnjRroqKiXOaMjo5WRkbGeXspLi6W0+l02QAAAHBtcOud5Mvx888/a9KkSRoyZIi8vb3N8XHjxunmm29WkyZNtGPHDiUmJuro0aOaPXu2JCkvL0+hoaEuc/n7+5vHGjdurLy8PHPs7Jq8vLzz9jN9+nRNmTKlui4PAAAAtchVEZLPnDmj++67T4Zh6LXXXnM5lpCQYH7duXNn2e12PfLII5o+fbocDscV6ykxMdHl3E6nU8HBwVfsfAAAAKg5tT4kVwTkb7/9Vps3b3a5i3wuERERKi0t1eHDh9W2bVsFBAQoPz/fpaZiv2Id8/lqzrfOWZIcDscVDeEAAABwn1q9JrkiIB84cECbNm1S06ZNL/qarKwseXh4yM/PT5IUGRmp9PR0nTlzxqxJTU1V27Zt1bhxY7MmLS3NZZ7U1FRFRkZW49UAAADgauHWO8lFRUU6ePCguZ+Tk6OsrCw1adJEgYGBuvfee/XFF19ozZo1KisrM9cIN2nSRHa7XRkZGfr000/Vu3dvNWrUSBkZGZowYYIefPBBMwA/8MADmjJliuLi4jRp0iTt3btX8+bN05w5c8zzPvHEE+rZs6dmzZql2NhYvfPOO9q5c6fLY+IAAADw6+HWkLxz50717t3b3K9Y4zt8+HAlJyfrgw8+kCR16dLF5XVbtmxRr1695HA49M477yg5OVnFxcUKDQ3VhAkTXNYK+/j4aOPGjYqPj1fXrl3VrFkzJSUlmY9/k6Rbb71Vy5Yt0+TJk/X000/rhhtu0OrVq9WpU6crePUAAACorWyGYRiX+6I+ffrovffeq/QoNqfTqYEDB2rz5s3V1d9Vw+l0ysfHR4WFhRddN30ldJ34Zo2fE0DNyJw5zN0tuEXu1DB3twDgCmmZtMct572cvFalNclbt25VSUlJpfGff/5Z//rXv6oyJQAAAFBrXNZyi927d5tff/XVVy7PES4rK9P69et13XXXVV93AAAAgBtcVkju0qWLbDabbDab+vTpU+l4vXr19Morr1RbcwAAAIA7XFZIzsnJkWEYuv766/XZZ5+pefPm5jG73S4/Pz95enpWe5MAAABATbqskBwSEiJJKi8vvyLNAAAAALVBlR8Bd+DAAW3ZskXHjh2rFJqTkpJ+cWMAAACAu1QpJP/tb3/TY489pmbNmikgIEA2m808ZrPZCMkAAAC4qlUpJD/33HN6/vnnNWnSpOruBwAAAHC7Kj0n+eTJk/rDH/5Q3b0AAAAAtUKVQvIf/vAHbdy4sbp7AQAAAGqFKi23aNOmjf785z/rk08+UVhYmOrWretyfNy4cdXSHAAAAOAOVQrJixYtUsOGDbVt2zZt27bN5ZjNZiMkAwAA4KpWpZCck5NT3X0AAAAAtUaV1iQDAAAA17Iq3Ul++OGHL3h88eLFVWoGAAAAqA2qFJJPnjzpsn/mzBnt3btXBQUF6tOnT7U0BgAAALhLlULyP//5z0pj5eXleuyxx9S6detf3BQAAADgTtW2JtnDw0MJCQmaM2dOdU0JAAAAuEW1fnDv0KFDKi0trc4pAQAAgBpXpeUWCQkJLvuGYejo0aNau3athg8fXi2NAQAAAO5SpZD85Zdfuux7eHioefPmmjVr1kWffAEAAADUdlUKyVu2bKnuPgAAAIBao0ohucLx48eVnZ0tSWrbtq2aN29eLU0BAAAA7lSlD+6dPn1aDz/8sAIDA9WjRw/16NFDQUFBiouL048//ljdPQIAAAA1qkohOSEhQdu2bdOHH36ogoICFRQU6P3339e2bdv05JNPVnePAAAAQI2q0nKLd999V6tWrVKvXr3MsTvvvFP16tXTfffdp9dee626+gMAAABqXJXuJP/444/y9/evNO7n58dyCwAAAFz1qhSSIyMj9eyzz+rnn382x3766SdNmTJFkZGR1dYcAAAA4A5VWm4xd+5c9e/fXy1atFB4eLgkadeuXXI4HNq4cWO1NggAAADUtCqF5LCwMB04cEBLly7V/v37JUlDhgzR0KFDVa9evWptEAAAAKhpVQrJ06dPl7+/v0aPHu0yvnjxYh0/flyTJk2qluYAAAAAd6jSmuS//vWvateuXaXxjh07auHChb+4KQAAAMCdqhSS8/LyFBgYWGm8efPmOnr06CXPk56ergEDBigoKEg2m02rV692OW4YhpKSkhQYGKh69eopKipKBw4ccKk5ceKEhg4dKm9vb/n6+iouLk5FRUUuNbt379Ydd9whLy8vBQcHa8aMGZV6Wblypdq1aycvLy+FhYVp3bp1l3wdAAAAuLZUKSQHBwdr+/btlca3b9+uoKCgS57n9OnTCg8P14IFC855fMaMGXr55Ze1cOFCffrpp2rQoIGio6NdnqoxdOhQ7du3T6mpqVqzZo3S09M1ZswY87jT6VS/fv0UEhKizMxMzZw5U8nJyVq0aJFZs2PHDg0ZMkRxcXH68ssvNXDgQA0cOFB79+695GsBAADAtaNKa5JHjx6t8ePH68yZM+rTp48kKS0tTU899dRl/ca9mJgYxcTEnPOYYRiaO3euJk+erN/97neSpDfffFP+/v5avXq1Bg8erK+//lrr16/X559/rltuuUWS9Morr+jOO+/USy+9pKCgIC1dulQlJSVavHix7Ha7OnbsqKysLM2ePdsM0/PmzVP//v01ceJESdK0adOUmpqq+fPns3wEAADgV6hKd5InTpyouLg4/fGPf9T111+v66+/Xo8//rjGjRunxMTEamksJydHeXl5ioqKMsd8fHwUERGhjIwMSVJGRoZ8fX3NgCxJUVFR8vDw0KeffmrW9OjRQ3a73ayJjo5Wdna2Tp48adacfZ6KmorznEtxcbGcTqfLBgAAgGtDlUKyzWbTX/7yFx0/flyffPKJdu3apRMnTigpKanaGsvLy5OkSr/Zz9/f3zyWl5cnPz8/l+N16tRRkyZNXGrONcfZ5zhfTcXxc5k+fbp8fHzMLTg4+HIvEQAAALVUlUJyhYYNG6pbt27q1KmTHA5HdfV0VUhMTFRhYaG5fffdd+5uCQAAANXkF4XkKykgIECSlJ+f7zKen59vHgsICNCxY8dcjpeWlurEiRMuNeea4+xznK+m4vi5OBwOeXt7u2wAAAC4NtTakBwaGqqAgAClpaWZY06nU59++qkiIyMlSZGRkSooKFBmZqZZs3nzZpWXlysiIsKsSU9P15kzZ8ya1NRUtW3bVo0bNzZrzj5PRU3FeQAAAPDr4taQXFRUpKysLGVlZUn674f1srKylJubK5vNpvHjx+u5557TBx98oD179mjYsGEKCgrSwIEDJUnt27dX//79NXr0aH322Wfavn27xo4dq8GDB5uPonvggQdkt9sVFxenffv2afny5Zo3b54SEhLMPp544gmtX79es2bN0v79+5WcnKydO3dq7NixNf2WAAAAoBao0iPgqsvOnTvVu3dvc78iuA4fPlwpKSl66qmndPr0aY0ZM0YFBQW6/fbbtX79enl5eZmvWbp0qcaOHau+ffvKw8NDgwYN0ssvv2we9/Hx0caNGxUfH6+uXbuqWbNmSkpKcnmW8q233qply5Zp8uTJevrpp3XDDTdo9erV6tSpUw28CwAAAKhtbIZhGO5u4lrgdDrl4+OjwsJCt6xP7jrxzRo/J4CakTlzmLtbcIvcqWHubgHAFdIyaY9bzns5ea3WrkkGAAAA3IWQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAotaH5FatWslms1Xa4uPjJUm9evWqdOzRRx91mSM3N1exsbGqX7++/Pz8NHHiRJWWlrrUbN26VTfffLMcDofatGmjlJSUmrpEAAAA1DJ13N3AxXz++ecqKysz9/fu3avf/va3+sMf/mCOjR49WlOnTjX369evb35dVlam2NhYBQQEaMeOHTp69KiGDRumunXr6oUXXpAk5eTkKDY2Vo8++qiWLl2qtLQ0jRo1SoGBgYqOjq6BqwQAAEBtUutDcvPmzV32X3zxRbVu3Vo9e/Y0x+rXr6+AgIBzvn7jxo366quvtGnTJvn7+6tLly6aNm2aJk2apOTkZNntdi1cuFChoaGaNWuWJKl9+/b6+OOPNWfOnPOG5OLiYhUXF5v7Tqfzl14qAAAAaolav9zibCUlJXrrrbf08MMPy2azmeNLly5Vs2bN1KlTJyUmJurHH380j2VkZCgsLEz+/v7mWHR0tJxOp/bt22fWREVFuZwrOjpaGRkZ5+1l+vTp8vHxMbfg4ODqukwAAAC4Wa2/k3y21atXq6CgQCNGjDDHHnjgAYWEhCgoKEi7d+/WpEmTlJ2drffee0+SlJeX5xKQJZn7eXl5F6xxOp366aefVK9evUq9JCYmKiEhwdx3Op0EZQAAgGvEVRWS//73vysmJkZBQUHm2JgxY8yvw8LCFBgYqL59++rQoUNq3br1FevF4XDI4XBcsfkBAADgPlfNcotvv/1WmzZt0qhRoy5YFxERIUk6ePCgJCkgIED5+fkuNRX7FeuYz1fj7e19zrvIAAAAuLZdNSF5yZIl8vPzU2xs7AXrsrKyJEmBgYGSpMjISO3Zs0fHjh0za1JTU+Xt7a0OHTqYNWlpaS7zpKamKjIyshqvAAAAAFeLqyIkl5eXa8mSJRo+fLjq1Pm/FSKHDh3StGnTlJmZqcOHD+uDDz7QsGHD1KNHD3Xu3FmS1K9fP3Xo0EEPPfSQdu3apQ0bNmjy5MmKj483l0s8+uij+uabb/TUU09p//79evXVV7VixQpNmDDBLdcLAAAA97oqQvKmTZuUm5urhx9+2GXcbrdr06ZN6tevn9q1a6cnn3xSgwYN0ocffmjWeHp6as2aNfL09FRkZKQefPBBDRs2zOW5yqGhoVq7dq1SU1MVHh6uWbNm6fXXX+cZyQAAAL9SV8UH9/r16yfDMCqNBwcHa9u2bRd9fUhIiNatW3fBml69eunLL7+sco8AAAC4dlwVd5IBAACAmkRIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAACLWh2Sk5OTZbPZXLZ27dqZx3/++WfFx8eradOmatiwoQYNGqT8/HyXOXJzcxUbG6v69evLz89PEydOVGlpqUvN1q1bdfPNN8vhcKhNmzZKSUmpicsDAABALVWrQ7IkdezYUUePHjW3jz/+2Dw2YcIEffjhh1q5cqW2bdumI0eO6J577jGPl5WVKTY2ViUlJdqxY4feeOMNpaSkKCkpyazJyclRbGysevfuraysLI0fP16jRo3Shg0bavQ6AQAAUHvUcXcDF1OnTh0FBARUGi8sLNTf//53LVu2TH369JEkLVmyRO3bt9cnn3yi7t27a+PGjfrqq6+0adMm+fv7q0uXLpo2bZomTZqk5ORk2e12LVy4UKGhoZo1a5YkqX379vr44481Z84cRUdH1+i1AgAAoHao9XeSDxw4oKCgIF1//fUaOnSocnNzJUmZmZk6c+aMoqKizNp27dqpZcuWysjIkCRlZGQoLCxM/v7+Zk10dLScTqf27dtn1pw9R0VNxRznU1xcLKfT6bIBAADg2lCrQ3JERIRSUlK0fv16vfbaa8rJydEdd9yhU6dOKS8vT3a7Xb6+vi6v8ff3V15eniQpLy/PJSBXHK84dqEap9Opn3766by9TZ8+XT4+PuYWHBz8Sy8XAAAAtUStXm4RExNjft25c2dFREQoJCREK1asUL169dzYmZSYmKiEhARz3+l0EpQBAACuEbX6TrKVr6+vbrzxRh08eFABAQEqKSlRQUGBS01+fr65hjkgIKDS0y4q9i9W4+3tfcEg7nA45O3t7bIBAADg2nBVheSioiIdOnRIgYGB6tq1q+rWrau0tDTzeHZ2tnJzcxUZGSlJioyM1J49e3Ts2DGzJjU1Vd7e3urQoYNZc/YcFTUVcwAAAODXp1aH5P/5n//Rtm3bdPjwYe3YsUO///3v5enpqSFDhsjHx0dxcXFKSEjQli1blJmZqZEjRyoyMlLdu3eXJPXr108dOnTQQw89pF27dmnDhg2aPHmy4uPj5XA4JEmPPvqovvnmGz311FPav3+/Xn31Va1YsUITJkxw56UDAADAjWr1muR///vfGjJkiH744Qc1b95ct99+uz755BM1b95ckjRnzhx5eHho0KBBKi4uVnR0tF599VXz9Z6enlqzZo0ee+wxRUZGqkGDBho+fLimTp1q1oSGhmrt2rWaMGGC5s2bpxYtWuj111/n8W8AAAC/YjbDMAx3N3EtcDqd8vHxUWFhoVvWJ3ed+GaNnxNAzcicOczdLbhF7tQwd7cA4AppmbTHLee9nLxWq5dbAAAAAO5ASAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAi1odkqdPn65u3bqpUaNG8vPz08CBA5Wdne1S06tXL9lsNpft0UcfdanJzc1VbGys6tevLz8/P02cOFGlpaUuNVu3btXNN98sh8OhNm3aKCUl5UpfHgAAAGqpWh2St23bpvj4eH3yySdKTU3VmTNn1K9fP50+fdqlbvTo0Tp69Ki5zZgxwzxWVlam2NhYlZSUaMeOHXrjjTeUkpKipKQksyYnJ0exsbHq3bu3srKyNH78eI0aNUobNmyosWsFAABA7VHH3Q1cyPr16132U1JS5Ofnp8zMTPXo0cMcr1+/vgICAs45x8aNG/XVV19p06ZN8vf3V5cuXTRt2jRNmjRJycnJstvtWrhwoUJDQzVr1ixJUvv27fXxxx9rzpw5io6OvnIXCAAAgFqpVt9JtiosLJQkNWnSxGV86dKlatasmTp16qTExET9+OOP5rGMjAyFhYXJ39/fHIuOjpbT6dS+ffvMmqioKJc5o6OjlZGRcd5eiouL5XQ6XTYAAABcG2r1neSzlZeXa/z48brtttvUqVMnc/yBBx5QSEiIgoKCtHv3bk2aNEnZ2dl67733JEl5eXkuAVmSuZ+Xl3fBGqfTqZ9++kn16tWr1M/06dM1ZcqUar1GAAAA1A5XTUiOj4/X3r179fHHH7uMjxkzxvw6LCxMgYGB6tu3rw4dOqTWrVtfsX4SExOVkJBg7judTgUHB1+x8wEAAKDmXBXLLcaOHas1a9Zoy5YtatGixQVrIyIiJEkHDx6UJAUEBCg/P9+lpmK/Yh3z+Wq8vb3PeRdZkhwOh7y9vV02AAAAXBtqdUg2DENjx47VP//5T23evFmhoaEXfU1WVpYkKTAwUJIUGRmpPXv26NixY2ZNamqqvL291aFDB7MmLS3NZZ7U1FRFRkZW05UAAADgalKrQ3J8fLzeeustLVu2TI0aNVJeXp7y8vL0008/SZIOHTqkadOmKTMzU4cPH9YHH3ygYcOGqUePHurcubMkqV+/furQoYMeeugh7dq1Sxs2bNDkyZMVHx8vh8MhSXr00Uf1zTff6KmnntL+/fv16quvasWKFZowYYLbrh0AAADuU6tD8muvvabCwkL16tVLgYGB5rZ8+XJJkt1u16ZNm9SvXz+1a9dOTz75pAYNGqQPP/zQnMPT01Nr1qyRp6enIiMj9eCDD2rYsGGaOnWqWRMaGqq1a9cqNTVV4eHhmjVrll5//XUe/wYAAPArVas/uGcYxgWPBwcHa9u2bRedJyQkROvWrbtgTa9evfTll19eVn8AAAC4NtXqO8kAAACAOxCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkGyxYMECtWrVSl5eXoqIiNBnn33m7pYAAABQwwjJZ1m+fLkSEhL07LPP6osvvlB4eLiio6N17Ngxd7cGAACAGkRIPsvs2bM1evRojRw5Uh06dNDChQtVv359LV682N2tAQAAoAbVcXcDtUVJSYkyMzOVmJhojnl4eCgqKkoZGRmV6ouLi1VcXGzuFxYWSpKcTueVb/Ycyop/cst5AVx57vq+4m6nfi5zdwsArhB3fV+rOK9hGBetJST/r++//15lZWXy9/d3Gff399f+/fsr1U+fPl1TpkypNB4cHHzFegTw6+TzyqPubgEAqtd0H7ee/tSpU/LxuXAPhOQqSkxMVEJCgrlfXl6uEydOqGnTprLZbG7sDNc6p9Op4OBgfffdd/L29nZ3OwDwi/F9DTXFMAydOnVKQUFBF60lJP+vZs2aydPTU/n5+S7j+fn5CggIqFTvcDjkcDhcxnx9fa9ki4ALb29v/jIBcE3h+xpqwsXuIFfgg3v/y263q2vXrkpLSzPHysvLlZaWpsjISDd2BgAAgJrGneSzJCQkaPjw4brlllv0m9/8RnPnztXp06c1cuRId7cGAACAGkRIPsv999+v48ePKykpSXl5eerSpYvWr19f6cN8gDs5HA49++yzlZb7AMDViu9rqI1sxqU8AwMAAAD4FWFNMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkA1eZBQsWqFWrVvLy8lJERIQ+++wzd7cEAFWSnp6uAQMGKCgoSDabTatXr3Z3S4CJkAxcRZYvX66EhAQ9++yz+uKLLxQeHq7o6GgdO3bM3a0BwGU7ffq0wsPDtWDBAne3AlTCI+CAq0hERIS6deum+fPnS/rvb4UMDg7W448/rj/96U9u7g4Aqs5ms+mf//ynBg4c6O5WAEncSQauGiUlJcrMzFRUVJQ55uHhoaioKGVkZLixMwAArj2EZOAq8f3336usrKzSb4D09/dXXl6em7oCAODaREgGAAAALAjJwFWiWbNm8vT0VH5+vst4fn6+AgIC3NQVAADXJkIycJWw2+3q2rWr0tLSzLHy8nKlpaUpMjLSjZ0BAHDtqePuBgBcuoSEBA0fPly33HKLfvOb32ju3Lk6ffq0Ro4c6e7WAOCyFRUV6eDBg+Z+Tk6OsrKy1KRJE7Vs2dKNnQE8Ag646syfP18zZ85UXl6eunTpopdfflkRERHubgsALtvWrVvVu3fvSuPDhw9XSkpKzTcEnIWQDAAAAFiwJhkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQBgatWqlebOnevuNgDA7QjJAPArlJKSIl9f30rjn3/+ucaMGVPzDVls3bpVNptNBQUF7m4FwK9UHXc3AACoPZo3b+7uFgCgVuBOMgDUUqtWrVJYWJjq1aunpk2bKioqSqdPn5Ykvf7662rfvr28vLzUrl07vfrqq+brDh8+LJvNpvfee0+9e/dW/fr1FR4eroyMDEn/vUs7cuRIFRYWymazyWazKTk5WVLl5RY2m01//etfddddd6l+/fpq3769MjIydPDgQfXq1UsNGjTQrbfeqkOHDrn0/v777+vmm2+Wl5eXrr/+ek2ZMkWlpaUu877++uv6/e9/r/r16+uGG27QBx98YPbfu3dvSVLjxo1ls9k0YsSI6n57AeDCDABArXPkyBGjTp06xuzZs42cnBxj9+7dxoIFC4xTp04Zb731lhEYGGi8++67xjfffGO8++67RpMmTYyUlBTDMAwjJyfHkGS0a9fOWLNmjZGdnW3ce++9RkhIiHHmzBmjuLjYmDt3ruHt7W0cPXrUOHr0qHHq1CnDMAwjJCTEmDNnjtmHJOO6664zli9fbmRnZxsDBw40WrVqZfTp08dYv3698dVXXxndu3c3+vfvb74mPT3d8Pb2NlJSUoxDhw4ZGzduNFq1amUkJye7zNuiRQtj2bJlxoEDB4xx48YZDRs2NH744QejtLTUePfddw1JRnZ2tnH06FGjoKCgZt54APhfhGQAqIUyMzMNScbhw4crHWvdurWxbNkyl7Fp06YZkZGRhmH8X0h+/fXXzeP79u0zJBlff/21YRiGsWTJEsPHx6fS3OcKyZMnTzb3MzIyDEnG3//+d3Ps7bffNry8vMz9vn37Gi+88ILLvP/4xz+MwMDA885bVFRkSDI++ugjwzAMY8uWLYYk4+TJk5V6BICawJpkAKiFwsPD1bdvX4WFhSk6Olr9+vXTvffeK7vdrkOHDikuLk6jR48260tLS+Xj4+MyR+fOnc2vAwMDJUnHjh1Tu3btLquXs+fx9/eXJIWFhbmM/fzzz3I6nfL29tauXbu0fft2Pf/882ZNWVmZfv75Z/3444+qX79+pXkbNGggb29vHTt27LJ6A4ArhZAMALWQp6enUlNTtWPHDm3cuFGvvPKKnnnmGX344YeSpL/97W+KiIio9Jqz1a1b1/zaZrNJksrLyy+7l3PNc6G5i4qKNGXKFN1zzz2V5vLy8jrnvBXzVKU/ALgSCMkAUEvZbDbddtttuu2225SUlKSQkBBt375dQUFB+uabbzR06NAqz22321VWVlaN3f6fm2++WdnZ2WrTpk2V57Db7ZJ0xXoEgIshJANALfTpp58qLS1N/fr1k5+fnz799FMdP35c7du315QpUzRu3Dj5+Piof//+Ki4u1s6dO3Xy5EklJCRc0vytWrVSUVGR0tLSFB4ervr165vLIH6ppKQk3XXXXWrZsqXuvfdeeXh4aNeuXdq7d6+ee+65S5ojJCRENptNa9as0Z133ql69eqpYcOG1dIfAFwKHgEHALWQt7e30tPTdeedd+rGG2/U5MmTNWvWLMXExGjUqFF6/fXXtWTJEoWFhalnz55KSUlRaGjoJc9/66236tFHH9X999+v5s2ba8aMGdXWe3R0tNasWaONGzeqW7du6t69u+bMmaOQkJBLnuO6667TlClT9Kc//Un+/v4aO3ZstfUHAJfCZhiG4e4mAAAAgNqEO8kAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFv8frFsd4Fy0igwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:43, 5.28MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:48<00:00, 8265.75it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 167MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000421673.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000310622.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000421673.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000421673.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000310622.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000310622.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000310622.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000310622.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000310622.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000421673.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000359276.jpg\n",
            "File not found: /content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000130712.jpg\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torchvision import models\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torchtext.vocab import GloVe\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "nltk.download('punkt')\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/sentiment/sentiment.csv\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=data, x='sentiment')\n",
        "plt.title('Distribution of Sentiments')\n",
        "plt.show()\n",
        "\n",
        "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "def clean_and_tokenize_caption(caption):\n",
        "    caption = re.sub(r'[^\\w\\s]', '', caption).lower()\n",
        "    words = word_tokenize(caption)\n",
        "    return words\n",
        "\n",
        "data['tokens'] = data['raw'].apply(clean_and_tokenize_caption)\n",
        "\n",
        "glove = GloVe(name='6B', dim=300)\n",
        "\n",
        "def tokens_to_embeddings(tokens, embedding=glove, embedding_dim=300, max_length=None):\n",
        "    if max_length is None:\n",
        "        max_length = max(len(t) for t in tokens)\n",
        "\n",
        "    embeddings = np.zeros((max_length, embedding_dim))\n",
        "\n",
        "    for i, token in enumerate(tokens[:max_length]):\n",
        "        embeddings[i] = embedding[token]\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # Changed to 224 to match with ResNet input\n",
        "    transforms.CenterCrop(224),  # Changed to 224 to match with ResNet input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pre-trained model\n",
        "cnn_model = models.resnet50(pretrained=True)\n",
        "# Remove last layer (we only want to extract features, not classify with this model)\n",
        "cnn_model = nn.Sequential(*list(cnn_model.children())[:-1])\n",
        "cnn_model.to(device)\n",
        "cnn_model.eval()  # Make sure to use eval mode\n",
        "\n",
        "\n",
        "def preprocess_and_extract_features(image_path):\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "        features = cnn_model(image_tensor)\n",
        "        return features.squeeze().cpu()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {image_path}\")\n",
        "        return torch.zeros(2048)  # ResNet50 outputs 2048-dimensional features\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['sentiment'])\n",
        "\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self, text_feature_dim, image_feature_dim, num_classes):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "\n",
        "        self.text_branch = nn.Sequential(\n",
        "            nn.Linear(text_feature_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.image_branch = nn.Sequential(\n",
        "            nn.Linear(image_feature_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_inputs, image_inputs):\n",
        "        text_features = self.text_branch(text_inputs)\n",
        "        text_features = text_features.mean(dim=1)\n",
        "\n",
        "        image_features = self.image_branch(image_inputs)\n",
        "\n",
        "        combined_features = torch.cat((text_features, image_features), dim=1)\n",
        "\n",
        "        output = self.classifier(combined_features)\n",
        "        return output\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "text_feature_dim = 300\n",
        "image_feature_dim = 2048  # Updated to match ResNet output\n",
        "num_classes = len(data['sentiment'].unique())\n",
        "model = MultiModalModel(text_feature_dim, image_feature_dim, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_folder, max_length, transform):\n",
        "        self.data = dataframe\n",
        "        self.image_folder = image_folder\n",
        "        self.max_length = max_length\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        tokens = clean_and_tokenize_caption(row['raw'])\n",
        "        text_features = tokens_to_embeddings(tokens, max_length=self.max_length)\n",
        "        text_features = torch.tensor(text_features).float()\n",
        "        image_path = os.path.join(self.image_folder, row['filename'])\n",
        "        image_features = preprocess_and_extract_features(image_path)\n",
        "        sentiment = torch.tensor(row['sentiment'])\n",
        "        return text_features, image_features, sentiment\n",
        "\n",
        "image_folder = \"/content/drive/MyDrive/sentiment/sentiment_images/\"\n",
        "max_length = data['tokens'].apply(len).max()\n",
        "batch_size = 16\n",
        "\n",
        "train_dataset = SentimentDataset(train_data, image_folder, max_length, transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = SentimentDataset(test_data, image_folder, max_length, transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (text_features, image_features, sentiment) in enumerate(train_dataloader):\n",
        "        text_features = text_features.to(device)\n",
        "        image_features = image_features.to(device)\n",
        "        sentiment = sentiment.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(text_features, image_features)\n",
        "        loss = criterion(outputs, sentiment)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_dataloader)}\")\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for text_features, image_features, sentiment in test_dataloader:\n",
        "        text_features = text_features.to(device)\n",
        "        image_features = image_features.to(device)\n",
        "        outputs = model(text_features, image_features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_true.extend(sentiment.tolist())\n",
        "        y_pred.extend(predicted.tolist())\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1_score:.4f}\")\n",
        "\n",
        "# Specify the file path for saving the model\n",
        "model_file = \"/content/drive/MyDrive/sentiment/multimodal_model.pth\"\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbYZIchT5ck8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Specify the file path for saving the model\n",
        "model_file = \"/content/drive/MyDrive/sentiment/multimodal_model.pth\"\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), model_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3tW8yjQ5jrE"
      },
      "source": [
        "PART2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tL1WaM-5k_R",
        "outputId": "76d2f6f2-26ba-4513-89f6-d6fd4a13d9ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU is available.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10, Loss: 0.02055501751601696\n",
            "Average test loss: 0.0192\n",
            "Test Accuracy of the model on the test images: 97.51661979033496 %\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet50\n",
        "from ast import literal_eval\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Data preprocessing\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/sentiment/sentiment.csv\")\n",
        "\n",
        "# Generate n-grams\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 5))\n",
        "X = vectorizer.fit_transform(data['text'])\n",
        "\n",
        "# Building a vocabulary\n",
        "vocab = {word: i + 4 for i, word in enumerate(vectorizer.get_feature_names_out())}\n",
        "vocab[\"<pad>\"] = 0\n",
        "vocab[\"<start>\"] = 1\n",
        "vocab[\"<end>\"] = 2\n",
        "vocab[\"<unk>\"] = 3\n",
        "\n",
        "# Pad sequences\n",
        "max_seq_length = X.shape[1]\n",
        "\n",
        "# Image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Dataset\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, path, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.path = path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data.iloc[idx][\"filename\"]\n",
        "        img_path = os.path.join(self.path, img_name)\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {img_path}\")\n",
        "            return None\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        text = torch.tensor(X[idx].toarray(), dtype=torch.float)\n",
        "\n",
        "        label = self.data.iloc[idx][\"sentiment\"]\n",
        "        label = torch.tensor(label, dtype=torch.float)\n",
        "\n",
        "        return img, text, label\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = MyDataset(train_data, \"/content/drive/MyDrive/sentiment/sentiment_images\", transform=transform)\n",
        "test_dataset = MyDataset(test_data, \"/content/drive/MyDrive/sentiment/sentiment_images\", transform=transform)\n",
        "\n",
        "class MultimodalModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, img_dim, hidden_dim, output_dim):\n",
        "        super(MultimodalModel, self).__init__()\n",
        "\n",
        "        self.text_layer = nn.Linear(vocab_size, embed_dim)\n",
        "\n",
        "        self.img_layer1 = models.resnet50(pretrained=True)\n",
        "        self.img_layer1.fc = nn.Linear(self.img_layer1.fc.in_features, img_dim)\n",
        "\n",
        "        self.final_layer1 = nn.Linear(img_dim + embed_dim, hidden_dim)\n",
        "        self.final_layer2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, img, text):\n",
        "        img_out = self.img_layer1(img)\n",
        "        text_out = self.text_layer(text)\n",
        "        combined = torch.cat((img_out, text_out), dim=1)\n",
        "\n",
        "        out = F.relu(self.final_layer1(combined))\n",
        "        out = self.final_layer2(out)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.01\n",
        "batch_size = 32\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 100\n",
        "img_dim = 1000\n",
        "hidden_dim = 500\n",
        "output_dim = 1  # change this to match the number of labels in your dataset\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"GPU is available.\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"GPU not available, CPU used.\")\n",
        "\n",
        "model = MultimodalModel(vocab_size, embed_dim, img_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "criterion = nn.MSELoss() # Use Mean Squared Error Loss for regression problem\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  for i, (images, texts, label) in enumerate(train_loader):\n",
        "    images = images.to(device)\n",
        "    texts = texts.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    outputs = model(images, texts)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = criterion(outputs, label)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "model.eval() # set the model to evaluation mode\n",
        "total_loss = 0\n",
        "total_items = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, (images, texts, label) in enumerate(test_loader):\n",
        "    images = images.to(device)\n",
        "    texts = texts.to(device)\n",
        "    label = label.to(device)\n",
        "        # Forward pass\n",
        "    outputs = model(images, texts)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = criterion(outputs, label)\n",
        "\n",
        "    total_loss += loss.item() * images.size(0)\n",
        "    total_items += images.size(0)\n",
        "avg_test_loss = total_loss / total_items\n",
        "print(f'Average test loss: {avg_test_loss:.4f}')\n",
        "\n",
        "model.eval() # Set the model to evaluation mode\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for i, (images, texts, label) in enumerate(test_loader):\n",
        "    images = images.to(device)\n",
        "    texts = texts.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images, texts)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Flatten the tensors and calculate the correct predictions\n",
        "    correct += (predicted.view(-1) == label.view(-1)).sum().item()\n",
        "\n",
        "    # Calculate the total number of items\n",
        "    total += label.nelement()\n",
        "\n",
        "    # Append true and predicted labels to the lists\n",
        "    true_labels.extend(label.view(-1).cpu().numpy())\n",
        "    pred_labels.extend(predicted.view(-1).cpu().numpy())\n",
        "\n",
        "print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbHjfUlD5qlM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Specify the file path for saving the model\n",
        "model_file = \"/content/drive/MyDrive/sentiment/mm2.pth\"\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPgsdqVE5_Wn",
        "outputId": "dd897ac5-2ada-44a5-9ae2-93faa83c47ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMK757HU5q5a"
      },
      "source": [
        "PART3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPjAAfi35rxU",
        "outputId": "3737eeec-fc44-4265-fc4c-1c01fe8784f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a photography of a little girl smiling while eating a piece of cake\n",
            "there is a little girl that is eating a piece of cake\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "\n",
        "img_url = '11.jpg'\n",
        "raw_image = Image.open(img_url).convert('RGB')\n",
        "\n",
        "# conditional image captioning\n",
        "text = \"a photography of\"\n",
        "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
        "\n",
        "out = model.generate(**inputs)\n",
        "print(processor.decode(out[0], skip_special_tokens=True))\n",
        "\n",
        "# unconditional image captioning\n",
        "inputs = processor(raw_image, return_tensors=\"pt\")\n",
        "\n",
        "out = model.generate(**inputs)\n",
        "print(processor.decode(out[0], skip_special_tokens=True))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zluhzjw6VG6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSZ7lWPE6EjR",
        "outputId": "d1bc3dda-bb15-4f5e-c217-2d03890de379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there is a plate of french fries and a sandwich on a table\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline\n",
        "\n",
        "# Initialize the captioning model\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "\n",
        "# Initialize the sentiment analysis model\n",
        "sentiment_model = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "img_url = '/content/drive/MyDrive/sentiment/sentiment_images/COCO_val2014_000000389081.jpg'\n",
        "raw_image = Image.open(img_url).convert('RGB')\n",
        "\n",
        "# Generate multiple captions\n",
        "inputs = processor(raw_image, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, num_return_sequences=10, num_beams=10)\n",
        "\n",
        "# Process each caption\n",
        "captions = [processor.decode(out, skip_special_tokens=True) for out in outputs]\n",
        "\n",
        "# Compute sentiment for each caption\n",
        "sentiments = [sentiment_model(caption)[0] for caption in captions]\n",
        "\n",
        "# Filter for positive captions\n",
        "positive_captions = [caption for caption, sentiment in zip(captions, sentiments) if sentiment['label'] == 'POSITIVE']\n",
        "\n",
        "# If there are no positive captions, just return the first caption\n",
        "if not positive_captions:\n",
        "    print(captions[0])\n",
        "else:\n",
        "    # Choose the most positive caption\n",
        "    max_positive_sentiment = max([sentiment['score'] for sentiment in sentiments if sentiment['label'] == 'POSITIVE'])\n",
        "    most_positive_caption = [caption for caption, sentiment in zip(positive_captions, sentiments) if sentiment['score'] == max_positive_sentiment]\n",
        "\n",
        "    print(most_positive_caption[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}